{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exhqcWQrri2T",
        "outputId": "d3f7f79e-ebe7-4e53-cb0b-fee6a3822b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "aptitude is already the newest version (0.8.13-3ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "mecab is already installed at the requested version (0.996-14build9)\n",
            "libmecab-dev is already installed at the requested version (0.996-14build9)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-3)\n",
            "git is already installed at the requested version (1:2.34.1-1ubuntu1.11)\n",
            "make is already installed at the requested version (4.3-4.1build1)\n",
            "curl is already installed at the requested version (7.81.0-1ubuntu1.17)\n",
            "xz-utils is already installed at the requested version (5.2.5-2ubuntu1)\n",
            "file is already installed at the requested version (1:5.41-3ubuntu0.1)\n",
            "mecab is already installed at the requested version (0.996-14build9)\n",
            "libmecab-dev is already installed at the requested version (0.996-14build9)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-3)\n",
            "git is already installed at the requested version (1:2.34.1-1ubuntu1.11)\n",
            "make is already installed at the requested version (4.3-4.1build1)\n",
            "curl is already installed at the requested version (7.81.0-1ubuntu1.17)\n",
            "xz-utils is already installed at the requested version (5.2.5-2ubuntu1)\n",
            "file is already installed at the requested version (1:5.41-3ubuntu0.1)\n",
            "No packages will be installed, upgraded, or removed.\n",
            "0 packages upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 0 B of archives. After unpacking 0 B will be used.\n",
            "                            \n",
            "Requirement already satisfied: mecab-python3==0.7 in /usr/local/lib/python3.10/dist-packages (0.7)\n"
          ]
        }
      ],
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lTjRYReVsc07"
      },
      "outputs": [],
      "source": [
        "import MeCab\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSqqu9enviPG",
        "outputId": "e5cab2cb-9967-46e4-86d4-ccba06591767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000 2500 2500\n"
          ]
        }
      ],
      "source": [
        "#Tokenisation\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "train_strings = []\n",
        "validation_strings = []\n",
        "test_strings = []\n",
        "train_labels = []\n",
        "validation_labels = []\n",
        "test_labels = []\n",
        "mecab = MeCab.Tagger(\"-Owakati\")\n",
        "for file_name, line_list, label_list in zip([\"train.txt\", \"validation.txt\", \"test.txt\"], [train_strings, validation_strings, test_strings], [train_labels, validation_labels, test_labels]):\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [line.strip().split(\"\\t\") for line in lines]\n",
        "        mecab = MeCab.Tagger(\"-Owakati\")\n",
        "        for i in range(len(lines)):\n",
        "            line = lines[i][0]\n",
        "            Writer_joy = lines[i][4] #Writer_joy signifies the level of joy of the sentence\n",
        "            line = mecab.parse(line).strip()\n",
        "            line_list.append(line)\n",
        "            if Writer_joy == \"0\":\n",
        "                label_list.append(0) #negative\n",
        "            else:\n",
        "                label_list.append(1) #positive\n",
        "\n",
        "train_strings, train_labels = shuffle(train_strings, train_labels)\n",
        "validation_strings, validation_labels = shuffle(validation_strings, validation_labels)\n",
        "test_strings, test_labels = shuffle(test_strings, test_labels)\n",
        "train_strings, train_labels = train_strings[:int(len(train_strings))], train_labels[:int(len(train_labels))]\n",
        "validation_strings, validation_labels = validation_strings[:int(len(validation_strings))], validation_labels[:int(len(validation_labels))]\n",
        "test_strings, test_labels = test_strings[:int(len(test_strings)/1)], test_labels[:int(len(test_labels)/1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(len(train_strings), len(validation_strings), len(test_strings))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBAY1UG70B-o",
        "outputId": "1893970b-9f40-48f1-d75a-60f5aa6efe8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "すごい スピード 感 0\n",
            "各人 が やり たい 仕事 を 自由 に 意見 でき 、 それ が ある程度 叶う 環境 で あれ ば まだ わかる が 、 まず お前 ら が その 環境 を 作る 努力 を し た の か と 。 夜勤 に ぶち込ん で 酷使 し とい て そんな 向上心 が 勝手 に 芽生える と 思う な よ と 。 0\n",
            "ちなみに 昨日 の 夜 は 今日 の 仕事 が 嫌 すぎ て アマゾン で ポチッ て しまっ た 。 の やっ さん と 大 魔王 とけん まち ゃんのねんどろいど … 。 振込 に 行か なく ちゃ … ♪ 1\n",
            "に ゃんにゃん ( 笑 ) 1\n",
            "仕事 いっぱい もらえ た ので 頑張り ます 1\n",
            "アカン 、 昨晩 から ずっと み ゅうみゅうと 子猫 の 鳴き声 が 響い て い て めちゃくちゃ 気 に なる … 何 も なけれ ば いい ん だ けど … 0\n",
            "掃除 は 無料 で スッキリ 感 を 味わえる エンタメ で ある 。 1\n",
            "謎 理論 0\n",
            "夢 の 中 で 悲しく て 辛い こと が あっ て ずっと ごめん 許し て っ て 泣い て 叫ん で た 気 が する 。 頭 が 痛い 。 0\n",
            "もう 勘弁 し て ください (‾▽‾;)\" 0\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(train_strings[i], train_labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN = [line.split(\" \") for line in train_strings]\n",
        "VALIDATION = [line.split(\" \") for line in validation_strings]\n",
        "TEST = [line.split(\" \") for line in test_strings]"
      ],
      "metadata": {
        "id": "CIurGQSzTotA"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "for i in range(len(TRAIN)):\n",
        "    total += len(TRAIN[i])\n",
        "print(total/len(TRAIN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igrdP2PmT4pS",
        "outputId": "e4ff73d1-fbfd-45db-b44d-dfd582a00b98"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19.282233333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UNAtbZM09MF"
      },
      "source": [
        "Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mbEZYIH30lC3"
      },
      "outputs": [],
      "source": [
        "#text vectorisation\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "text_vectorization = layers.TextVectorization(max_tokens=20000, ngrams=2, output_mode=\"multi_hot\", split=\"whitespace\")\n",
        "text_vectorization.adapt(train_strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKfhDQMm3wTQ",
        "outputId": "11bbe25a-e98e-4119-837e-c7b72f2ba323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 383ms/step - accuracy: 0.6191 - loss: 0.6624 - val_accuracy: 0.6008 - val_loss: 0.6979\n",
            "Epoch 2/10\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 379ms/step - accuracy: 0.7352 - loss: 0.5442 - val_accuracy: 0.6388 - val_loss: 0.6894\n",
            "Epoch 3/10\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 385ms/step - accuracy: 0.7791 - loss: 0.4843 - val_accuracy: 0.6376 - val_loss: 0.7210\n",
            "Epoch 4/10\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 382ms/step - accuracy: 0.8064 - loss: 0.4366 - val_accuracy: 0.6380 - val_loss: 0.7486\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def prepare_dataset(texts, labels, batch_size=256):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    dataset = dataset.map(lambda x, y: (text_vectorization(x), y))\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = prepare_dataset(train_strings, train_labels)\n",
        "validation_dataset = prepare_dataset(validation_strings, validation_labels)\n",
        "\n",
        "\n",
        "max_tokens = 20000\n",
        "inputs = keras.Input(shape=(max_tokens,),)\n",
        "x = layers.Dense(16, activation=\"relu\")(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "GLLlOiGuKEri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lanzlCGJ4_G5"
      },
      "outputs": [],
      "source": [
        "#text vectorisation\n",
        "text_vectorization = layers.TextVectorization(max_tokens=20000, ngrams=2, output_mode=\"tf_idf\", split=\"whitespace\")\n",
        "text_vectorization.adapt(train_strings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = prepare_dataset(train_strings, train_labels)\n",
        "validation_dataset = prepare_dataset(validation_strings, validation_labels)\n",
        "\n",
        "\n",
        "max_tokens = 20000\n",
        "inputs = keras.Input(shape=(max_tokens,),)\n",
        "x = layers.Dense(16, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C89aA5xKO9-",
        "outputId": "9c684d61-558a-4e09-db3b-38c9b7b313ff"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 383ms/step - accuracy: 0.6207 - loss: 0.6575 - val_accuracy: 0.5920 - val_loss: 0.7363\n",
            "Epoch 2/10\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 383ms/step - accuracy: 0.7542 - loss: 0.5186 - val_accuracy: 0.6284 - val_loss: 0.7240\n",
            "Epoch 3/10\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 378ms/step - accuracy: 0.8129 - loss: 0.4269 - val_accuracy: 0.6356 - val_loss: 0.7901\n",
            "Epoch 4/10\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 380ms/step - accuracy: 0.8455 - loss: 0.3602 - val_accuracy: 0.6260 - val_loss: 0.9466\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}